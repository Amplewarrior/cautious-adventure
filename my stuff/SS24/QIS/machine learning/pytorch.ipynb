{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this cell imports everything you need\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradients tell our models which direction to go. We are following [this video](https://youtu.be/c36lUUr864M?si=4P2y2N8_1GwBNANe)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 w = 1.800 loss = 30.00000000\n",
      "epoch 1 w = 1.980 loss = 0.30000019\n",
      "epoch 2 w = 1.998 loss = 0.00299999\n",
      "epoch 3 w = 2.000 loss = 0.00003000\n",
      "epoch 4 w = 2.000 loss = 0.00000030\n",
      "epoch 5 w = 2.000 loss = 0.00000000\n",
      "epoch 6 w = 2.000 loss = 0.00000000\n",
      "epoch 7 w = 2.000 loss = 0.00000000\n",
      "epoch 8 w = 2.000 loss = 0.00000000\n",
      "epoch 9 w = 2.000 loss = 0.00000000\n"
     ]
    }
   ],
   "source": [
    "x = np.array([1,2,3,4], dtype=np.float32)\n",
    "y = np.array([2,4,6,8], dtype=np.float32)\n",
    "\n",
    "w = 0.0\n",
    "\n",
    "def forward(x): #helps us make progress\n",
    "    return w * x\n",
    "\n",
    "def loss(y,y_predicted): #knowing how far off we are tells us where to go\n",
    "    return((y_predicted-y)**2).mean()\n",
    "\n",
    "#If we wanted to do this without pytorch:\n",
    "#error is (wx-y)^2\n",
    "#We can do the derivative ourselves derror/dw=2x(wx-y)\n",
    "#in code this looks like:\n",
    "\n",
    "def humangrad(x,y,y_predicted):\n",
    "    return np.dot(2*x,y_predicted-y).mean() #this is the derivative of loss\n",
    "\n",
    "learning_rate = 0.015 #really this means how much we're affected by our grad\n",
    "n_iters = 10 #times we run \n",
    "\n",
    "for epoch in range(n_iters): #epoch is time when training is doen\n",
    "    y_pred = forward(x) #from previous epoch\n",
    "\n",
    "    dw = humangrad(x,y,y_pred) #dw drops as y_pred approaches y. x doesn't change\n",
    "\n",
    "    l = loss(y, y_pred) #its interesting that loss isn't needed to train the model?\n",
    "    w -= learning_rate * dw #moving in the right direction, don't overshoot it\n",
    "\n",
    "    if epoch % 1 ==0: #shouldn't epoch always be an integer?\n",
    "        print(f'epoch {epoch} w = {w:.3f} loss = {l:.8f}')\n",
    "\n",
    "#remember w is suppposed to be 2.0 since we want to have xw=y with a known x and unknown y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets do this with the tools at our disposal with pytorch!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 w = 0.300 loss = 30.00000000\n",
      "epoch 11 w = 1.665 loss = 1.16278565\n",
      "epoch 21 w = 1.934 loss = 0.04506890\n",
      "epoch 31 w = 1.987 loss = 0.00174685\n",
      "epoch 41 w = 1.997 loss = 0.00006770\n",
      "epoch 51 w = 1.999 loss = 0.00000262\n",
      "epoch 61 w = 2.000 loss = 0.00000010\n",
      "epoch 71 w = 2.000 loss = 0.00000000\n",
      "epoch 81 w = 2.000 loss = 0.00000000\n",
      "epoch 91 w = 2.000 loss = 0.00000000\n",
      "epoch 101 w = 2.000 loss = 0.00000000\n",
      "epoch 111 w = 2.000 loss = 0.00000000\n",
      "epoch 121 w = 2.000 loss = 0.00000000\n",
      "epoch 131 w = 2.000 loss = 0.00000000\n",
      "epoch 141 w = 2.000 loss = 0.00000000\n",
      "epoch 151 w = 2.000 loss = 0.00000000\n",
      "epoch 161 w = 2.000 loss = 0.00000000\n",
      "epoch 171 w = 2.000 loss = 0.00000000\n",
      "epoch 181 w = 2.000 loss = 0.00000000\n",
      "epoch 191 w = 2.000 loss = 0.00000000\n"
     ]
    }
   ],
   "source": [
    "#x = np.array([1,2,3,4], dtype=np.float32)\n",
    "#y = np.array([2,4,6,8], dtype=np.float32)\n",
    "\n",
    "x = torch.tensor([1,2,3,4], dtype=torch.float32)\n",
    "y = torch.tensor([2,4,6,8], dtype=torch.float32)\n",
    "\n",
    "w = torch.tensor(0.0, dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "def forward(x): #helps us make progress\n",
    "    return w * x\n",
    "\n",
    "def loss(y,y_predicted): #knowing how far off we are tells us where to go\n",
    "    return((y_predicted-y)**2).mean()\n",
    "\n",
    "#now gradient comes from pytorch. loss comes into\n",
    "\n",
    "\n",
    "learning_rate = 0.01 #really this means how much we're affected by our grad\n",
    "n_iters = 200 #times we run \n",
    "\n",
    "for epoch in range(n_iters): #epoch is time when training is doen\n",
    "    y_pred = forward(x) #from previous epoch\n",
    "\n",
    "    \n",
    "    l = loss(y, y_pred) #its interesting that loss isn't needed to train the model?\n",
    "    \n",
    "    l.backward() #it knows its w because of requires_grad=True\n",
    "    #this sends w its grad back\n",
    "    #apparently backward() isn't as numerically accurate as the direct definition\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        w -= learning_rate * w.grad #moving in the right direction, don't overshoot it\n",
    "\n",
    "    #we don't want to accumulate our gradients\n",
    "    w.grad.zero_() #in_place\n",
    "\n",
    "    if epoch % 10 ==0: #shouldn't epoch always be an integer?\n",
    "        print(f'epoch {epoch+1} w = {w:.3f} loss = {l:.8f}')\n",
    "\n",
    "#remember w is suppposed to be 2.0 since we want to have xw=y with a known x and unknown y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pytorch optimizer and pytorch model, the whole pipeline\n",
    "\n",
    "1. design model\n",
    "2. Construct loss and optimizer\n",
    "3. training loop\n",
    "    - forward pass: predict values\n",
    "    - backward pass: see the right direction \n",
    "    <!-- I know its weird these are backwards -->\n",
    "    - new: updating weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.],\n",
       "        [2.],\n",
       "        [3.],\n",
       "        [4.]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([1,2,3,4], dtype=torch.float32)\n",
    "x.view(4,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 w = 4.986 loss = 52.24533844\n",
      "epoch 11 w = 213.242 loss = 184135.40625000\n",
      "epoch 21 w = 12624.092 loss = 655232512.00000000\n",
      "epoch 31 w = 752955.438 loss = 2331597864960.00000000\n",
      "epoch 41 w = 44915612.000 loss = 8296825085755392.00000000\n",
      "epoch 51 w = 2679331840.000 loss = 29523664016576086016.00000000\n",
      "epoch 61 w = 159829000192.000 loss = 105057846408274811813888.00000000\n",
      "epoch 71 w = 9534217125888.000 loss = 373841346462916298858823680.00000000\n",
      "epoch 81 w = 568740710187008.000 loss = 1330288522604194731097107464192.00000000\n",
      "epoch 91 w = 33926852909203456.000 loss = 4733740347122776352482637633290240.00000000\n",
      "epoch 101 w = 2023824324003102720.000 loss = 16844689490952938989928167307311316992.00000000\n",
      "epoch 111 w = 120726367933711777792.000 loss = inf\n",
      "epoch 121 w = 7201640298983609860096.000 loss = inf\n",
      "epoch 131 w = 429596603043817684205568.000 loss = inf\n",
      "epoch 141 w = 25626548908590635876876288.000 loss = inf\n",
      "epoch 151 w = 1528689650381750384944218112.000 loss = inf\n",
      "epoch 161 w = 91190266270492881273581731840.000 loss = inf\n",
      "epoch 171 w = 5439733997285319056247834542080.000 loss = inf\n",
      "epoch 181 w = 324494104342650840522726762872832.000 loss = inf\n",
      "epoch 191 w = 19356907061181330849728542529290240.000 loss = inf\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "x = torch.tensor([1,2,3,4], dtype=torch.float32)\n",
    "x = x.view(4,1)\n",
    "\n",
    "y = torch.tensor([2,4,6,8], dtype=torch.float32)\n",
    "y = y.view(4,1)\n",
    "\n",
    "X_test = torch.tensor([5],dtype=torch.float32) #IVP\n",
    "n_samples, n_features = x.shape\n",
    "\n",
    "input_size = n_features #why we reshaped it\n",
    "output_size = n_features\n",
    "\n",
    "model = nn.Linear(input_size,output_size) #built in model, based on neural network.\n",
    "#I'll have to read how pytorch's neural networks work\n",
    "\n",
    "\n",
    "learning_rate = 0.01\n",
    "n_iters = 2000  \n",
    "\n",
    "loss = nn.MSELoss() #I'll have to read how these models work\n",
    "optimizer = torch.optim.SGD(model.parameters(),lr=learning_rate) #changes the value of w\n",
    "\n",
    "for epoch in range(n_iters): \n",
    "    y_pred = model(x) \n",
    "    \n",
    "    l = loss(y, y_pred) #sticks around\n",
    "\n",
    "    l.backward() \n",
    "    \n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad() \n",
    "\n",
    "    if epoch % 200 ==0: \n",
    "        [w,b]=model.parameters()\n",
    "        print(f'epoch {epoch+1} w = {w[0][0].item():.3f} loss = {l:.8f}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lets define our own model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegression(nn.Module):\n",
    "\n",
    "    def __init__(self,input_dim,output_dim):\n",
    "        super(LinearRegression, self).__init__()\n",
    "        #idk layers or smth\n",
    "        self.lin = nn.Linear(input_dim,output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.lin(x) #I have no clue whats going on\n",
    "    \n",
    "#whats a class?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
